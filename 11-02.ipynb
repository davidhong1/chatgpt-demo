{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 使用 ChatGLM 提供对话效果"
      ],
      "metadata": {
        "id": "NJSnV6W9CTTV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install icetk\n",
        "!pip install cpm_kernels"
      ],
      "metadata": {
        "id": "PoB45L1G7LQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BYVCp4sT6r1R"
      },
      "outputs": [],
      "source": [
        "\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm-6b-int4\", trust_remote_code=True)\n",
        "model = AutoModel.from_pretrained(\"THUDM/chatglm-6b-int4\", trust_remote_code=True).half().cuda()\n",
        "model = model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding\n",
        "\n",
        "print(locale.getpreferredencoding())\n",
        "\n",
        "!pip install --upgrade protobuf"
      ],
      "metadata": {
        "id": "UdcDRXto9x3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "question = \"\"\"\n",
        "自收到商品之日起7天内，如产品未使用、包装完好，您可以申请退货。某些特殊商品可能不支持退货，请在购买前查看商品详情页面的退货政策。\n",
        "\n",
        "根据以上信息，请回答下面的问题：\n",
        "\n",
        "Q: 你们的退货政策是怎么样的？\n",
        "\"\"\"\n",
        "response, history = model.chat(tokenizer, question, history=[])\n",
        "print(response)"
      ],
      "metadata": {
        "id": "pXe8BXHK-Go7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "question = \"\"\"\n",
        "Q: 你们的退货政策是怎么样的？\n",
        "A: \n",
        "\"\"\"\n",
        "response, history = model.chat(tokenizer, question, history=[])\n",
        "print(response)"
      ],
      "metadata": {
        "id": "LCsAiXzk-6rP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "question = \"\"\"\n",
        "我们支持全国大部分省份的配送，包括北京、上海、天津、重庆、河北、山西、辽宁、吉林、黑龙江、江苏、浙江、安徽、福建、江西、山东、河南、湖北、湖南、广东、海南、四川、贵州、云南、陕西、甘肃、青海、台湾、内蒙古、广西、西藏、宁夏和新疆.\n",
        "\n",
        "根据以上信息，请回答下面的问题：\n",
        "\n",
        "Q: 你们能配送到三亚吗？\n",
        "\"\"\"\n",
        "response, history = model.chat(tokenizer, question, history=[])\n",
        "print(response)"
      ],
      "metadata": {
        "id": "coupuFDYAvLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "question = \"\"\"\n",
        "我们支持全国大部分省份的配送，包括北京、上海、天津、重庆、河北、山西、江苏、浙江、安徽、福建、江西、山东、河南、湖北、湖南、广东、海南、四川、贵州、云南、陕西、甘肃、青海、台湾、内蒙古、广西、西藏、宁夏和新疆.但是不能配送到东三省\n",
        "\n",
        "根据以上信息，请回答下面的问题：\n",
        "\n",
        "Q: 你们能配送到哈尔滨吗？\n",
        "\"\"\"\n",
        "response, history = model.chat(tokenizer, question, history=[])\n",
        "print(response)"
      ],
      "metadata": {
        "id": "oD1qcQPSA1vH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 将 ChatGLM 封装成 LLM"
      ],
      "metadata": {
        "id": "WxMM6dDrCc91"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai\n",
        "!pip install faiss-gpu\n",
        "!pip install llama_index\n",
        "!pip install langchain"
      ],
      "metadata": {
        "id": "m0OyL0atCo7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import openai, os\n",
        "import faiss\n",
        "from llama_index import SimpleDirectoryReader, LangchainEmbedding, ServiceContext, GPTVectorStoreIndex, StorageContext\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from llama_index.node_parser import SimpleNodeParser\n",
        "\n",
        "from langchain.llms.base import LLM\n",
        "from llama_index import LLMPredictor\n",
        "from typing import Optional, List, Mapping, Any\n",
        "\n",
        "class CustomLLM(LLM):\n",
        "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
        "        response, history = model.chat(tokenizer, prompt, history=[])\n",
        "        return response\n",
        "\n",
        "    @property\n",
        "    def _identifying_params(self) -> Mapping[str, Any]:\n",
        "        return {\"name_of_model\": \"chatglm-6b-int4\"}\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"custom\""
      ],
      "metadata": {
        "id": "PMB5ca9ACjuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence_transformers"
      ],
      "metadata": {
        "id": "K2LutnPVLp48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain.text_splitter import SpacyTextSplitter\n",
        "from llama_index.vector_stores.faiss import FaissVectorStore\n",
        "\n",
        "llm_predictor = LLMPredictor(llm=CustomLLM())\n",
        "\n",
        "text_splitter = CharacterTextSplitter(separator=\"\\n\\n\", chunk_size=100, chunk_overlap=20)\n",
        "parser = SimpleNodeParser(text_splitter=text_splitter)\n",
        "documents = SimpleDirectoryReader('./drive/MyDrive/colab_data/faq/').load_data()\n",
        "nodes = parser.get_nodes_from_documents(documents)\n",
        "\n",
        "embed_model = LangchainEmbedding(HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\n",
        "))\n",
        "service_context = ServiceContext.from_defaults(embed_model=embed_model, llm_predictor=llm_predictor)\n",
        "\n",
        "dimension = 768\n",
        "faiss_index = faiss.IndexFlatIP(dimension)\n",
        "\n",
        "vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "index = GPTVectorStoreIndex.from_documents(documents, service_context=service_context)\n"
      ],
      "metadata": {
        "id": "DzuiytJpHTMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from llama_index import QuestionAnswerPrompt\n",
        "\n",
        "QA_PROMPT_TMPL = (\n",
        "    \"{context_str}\"\n",
        "    \"\\n\\n\"\n",
        "    \"根据以上信息，请回答下面的问题：\\n\"\n",
        "    \"Q: {query_str}\\n\"\n",
        "    )\n",
        "QA_PROMPT = QuestionAnswerPrompt(QA_PROMPT_TMPL)\n",
        "\n",
        "query_engine = index.as_query_engine(text_qa_template=QA_PROMPT, verbose=True)\n",
        "response = query_engine.query(\"请问你们海南能发货吗？\")\n",
        "print(response)\n"
      ],
      "metadata": {
        "id": "_34SkFpwMD1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 开源模型的不足之处"
      ],
      "metadata": {
        "id": "ZHpX7bV8MkcO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download zh_core_web_sm"
      ],
      "metadata": {
        "id": "Hkq6OXZwNsGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "text_splitter = SpacyTextSplitter(pipeline=\"zh_core_web_sm\", chunk_size = 128, chunk_overlap=32)\n",
        "parser = SimpleNodeParser(text_splitter=text_splitter)\n",
        "documents = SimpleDirectoryReader('./drive/MyDrive/colab_data/zhaohuaxishi/').load_data()\n",
        "nodes = parser.get_nodes_from_documents(documents)\n",
        "\n",
        "embed_model = LangchainEmbedding(HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\n",
        "))\n",
        "service_context = ServiceContext.from_defaults(embed_model=embed_model, llm_predictor=llm_predictor)\n",
        "\n",
        "dimension = 768\n",
        "faiss_index = faiss.IndexFlatIP(dimension)\n",
        "\n",
        "vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "index = GPTVectorStoreIndex.from_documents(documents, service_context=service_context)\n"
      ],
      "metadata": {
        "id": "EgqeTgS4Mik_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# query will use the same embed_model\n",
        "from llama_index import QuestionAnswerPrompt\n",
        "\n",
        "# openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "QA_PROMPT_TMPL = (\n",
        "    \"下面的内容来自鲁迅先生的散文集《朝花夕拾》，很多内容是以第一人称写的 \\n\"\n",
        "    \"---------------------\\n\"\n",
        "    \"{context_str}\"\n",
        "    \"\\n---------------------\\n\"\n",
        "    \"根据这些信息，请回答问题: {query_str}\\n\"\n",
        "    \"如果您不知道的话，请回答不知道\\n\"\n",
        ")\n",
        "QA_PROMPT = QuestionAnswerPrompt(QA_PROMPT_TMPL)\n",
        "\n",
        "query_engine = index.as_query_engine(similarity_top_k = 1, text_qa_template=QA_PROMPT, verbose=True)\n",
        "response = query_engine.query(\"鲁迅先生在日本学习医学的老师是谁？\")\n",
        "print(response)\n"
      ],
      "metadata": {
        "id": "pPYgzrGROCUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\"鲁迅先生是在日本的哪个城市学习医学的？\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "bBFGhAEoOgfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "question = \"\"\"Consideration proudct : 工厂现货PVC充气青蛙夜市地摊热卖充气玩具发光蛙儿童水上玩具\n",
        "\n",
        "1. Compose human readale product title used on Amazon in english within 20 words.\n",
        "2. Write 5 selling points for the products in Amazon.\n",
        "3. Evaluate a price range for this product in U.S.\n",
        "\n",
        "Output the result in json format with three properties called title, selling_points and price_range\"\"\"\n",
        "response, history = model.chat(tokenizer, question, history=[])\n",
        "print(response)"
      ],
      "metadata": {
        "id": "rXsygwBCOrVX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}